{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0009826",
   "metadata": {},
   "source": [
    "## Kuro Siwo dataset exploration\n",
    "\n",
    "This notebook showcases the data included in Kuro Siwo and provides visualised examples of different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af4d3f",
   "metadata": {},
   "source": [
    "We assume that this notebook is run from inside the [Kuro Siwo repository](https://github.com/Orion-AI-Lab/KuroSiwo) and that the dataset, along with the pickle and JSON files with the train/val/test splits, have been downloaded inside a particular folder of the user's choice.\n",
    "\n",
    "First, let's import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c773630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyjson5 as json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from utilities.utilities import (\n",
    "    prepare_loaders,\n",
    "    update_config,\n",
    "    reverse_scale_img,\n",
    "    PROJ_ROOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db398be1-7a5c-4ddb-92d5-40143f1333d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA avail? False\n",
      "GPU: None\n"
     ]
    }
   ],
   "source": [
    "# Sanity\n",
    "import os\n",
    "\n",
    "print(\"CUDA avail?\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e521b35",
   "metadata": {},
   "source": [
    "### Configuration files\n",
    "\n",
    "An important part of the Kuro Siwo implementation is the configuration files. There are several different files, all residing inside the `configs` folder:\n",
    "\n",
    " - **augmentations/** contains configurations for the data augmentations during training\n",
    " - **loss/** contains specific configurations for some loss functions\n",
    " - **method/** contains configurations for the particular model to be run\n",
    " - **train/** contains configurations focused on the training setup and the data to be used\n",
    " - **config.json** contains generic configuration parameters for the whole pipeline\n",
    " \n",
    "Let's open the `config.json` file and see its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1740bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'segmentation',\n",
       " 'root_path': 'KuroSiwo/',\n",
       " 'wandb_project': 'Your_Project',\n",
       " 'wandb_entity': 'your_entity',\n",
       " 'wandb_id': 'None',\n",
       " 'resume_wandb': False,\n",
       " 'wandb_activate': False,\n",
       " 'log_AOI_metrics': True,\n",
       " 'log_zone_metrics': False,\n",
       " 'method': 'unet',\n",
       " 'gpu': 0,\n",
       " 'mixed_precision': True,\n",
       " 'num_classes': 3,\n",
       " 'test': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = json.load(open(\"configs/config.json\", \"r\"))\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5b9b0",
   "metadata": {},
   "source": [
    "Let's explain some of the parameters:\n",
    "\n",
    " - `task`: can be either `segmentation`, `cd` for change detection, or `mae` for Masked Autoencoder\n",
    " - `root_path`: the path containing the dataset\n",
    " - `method`: the model to be used\n",
    " - `gpu`: the GPU id to be used\n",
    " - `num_classes`: the number of classes. Here we use 3: no water, permanent water, flood\n",
    " - `test`: of true, then the model runs in inference mode. Otherwise, the model runs in training mode\n",
    " \n",
    "Let's update the `root_path` with the folder containing Kuro Siwo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e338a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"root_path\"] = PROJ_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5107d5c",
   "metadata": {},
   "source": [
    " Now, the other most important configuration files reside inside the **configs/train/** folder. Let's open them to see their contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track': 'RandomEvents',\n",
       " 'train_pickle': 'pickle/KuroV2_grid_dict.gz',\n",
       " 'test_pickle': 'pickle/KuroV2_grid_dict_test_0_100.gz',\n",
       " 'negative_pickle': None,\n",
       " 'slc': False,\n",
       " 'train_json': 'json/slc_grid_pwater_0.0001.json',\n",
       " 'test_json': 'json/slc_grid_pwater_0.json',\n",
       " 'slc_root_path': '',\n",
       " 'inputs': ['pre_event_1', 'pre_event_2', 'post_event'],\n",
       " 'channels': ['vv', 'vh'],\n",
       " 'water_percentage': '[0,100]',\n",
       " 'data_augmentations': False,\n",
       " 'clamp_input': 0.15,\n",
       " 'scale_input': 'normalize',\n",
       " 'data_mean': [0.0953, 0.0264],\n",
       " 'data_std': [0.0427, 0.0215],\n",
       " 'dem_mean': 93.4313,\n",
       " 'dem_std': 1410.8382,\n",
       " 'slc_dem_mean': 82.96274925580951,\n",
       " 'slc_dem_std': 153.71243439980663,\n",
       " 'slc_mean': [0.022367, 39.242, 81.13, 0.043526],\n",
       " 'slc_std': [1.2843, 25.6152, 58.0151, 1.2844],\n",
       " 'dem': False,\n",
       " 'slope': False,\n",
       " 'slope_mean': 2.1277,\n",
       " 'slope_std': 67.5048,\n",
       " 'slc_slope_mean': 0.3977,\n",
       " 'slc_slope_std': 0.4946,\n",
       " 'reverse_scaling': False,\n",
       " 'uint8': False,\n",
       " 'train_acts': [130,\n",
       "  470,\n",
       "  555,\n",
       "  118,\n",
       "  174,\n",
       "  324,\n",
       "  421,\n",
       "  554,\n",
       "  427,\n",
       "  518,\n",
       "  502,\n",
       "  498,\n",
       "  497,\n",
       "  496,\n",
       "  492,\n",
       "  147,\n",
       "  267,\n",
       "  273,\n",
       "  275,\n",
       "  417,\n",
       "  567,\n",
       "  1111011,\n",
       "  1111004,\n",
       "  1111009,\n",
       "  1111010,\n",
       "  1111006,\n",
       "  1111005],\n",
       " 'val_acts': [514, 559, 279, 520, 437, 1111003, 1111008],\n",
       " 'test_acts': [321, 561, 445, 562, 411, 1111002, 277, 1111007, 205, 1111013]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_configs = json.load(open(PROJ_ROOT / \"configs/train/data_config.json\", \"r\"))\n",
    "data_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a87c83",
   "metadata": {},
   "source": [
    "Let's explain some of the parameters:\n",
    "\n",
    " - `track`: this should always be \"RandomEvents\"\n",
    " - `train_pickle`: the path of the pickle file for the train split of the GRD data\n",
    " - `test_pickle`: the path of the pickle file for the test split of the GRD data\n",
    " - `slope_path`: the path of the precomputed slopes (optional. If not given slope is computed on the fly)\n",
    " - `slc`: if True, then SLC data are used. Otherwise, GRD data are used.\n",
    " - `train_json`: the path of the JSON file for the train split of SLC data\n",
    " - `test_json`: the path of the JSON file for the test split of SLC data \n",
    " - `slc_root_path`: the path containing the SLC data\n",
    " - `inputs`: a list containing at least one of the following: \"pre_event_1\", \"pre_event_2\", \"post_event\"\n",
    " - `channels`: a list containing at least one of the following: \"vv\", \"vh\"\n",
    " - `data_augmentations`: if true, then data augmentations are used during training\n",
    " - `dem`: if true, then DEM data are also used\n",
    " - `slope`: if true, then slope data are also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'epochs': 1,\n",
       " 'num_workers': 8,\n",
       " 'start_epoch': 0,\n",
       " 'print_frequency': 10,\n",
       " 'on_screen_prints': False,\n",
       " 'train_save_checkpoint_freq': 1,\n",
       " 'weighted': False,\n",
       " 'resume_checkpoint': False,\n",
       " 'loss_function': 'cross_entropy',\n",
       " 'oversampling': False,\n",
       " 'evaluate_water': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_configs = json.load(open(PROJ_ROOT / \"configs/train/train_config.json\", \"r\"))\n",
    "train_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85564e82",
   "metadata": {},
   "source": [
    "Let's explain some of the parameters:\n",
    "\n",
    " - `batch_size`: the size of the batch\n",
    " - `epochs`: the number of training epochs\n",
    " - `num_workers`: the number of workers to be used for the dataset\n",
    " - `print_frequency`: the number of epochs after which to print informative messages\n",
    " - `on_screen_prints`: if true, then informative messages are printed on the screen\n",
    " - `train_save_checkpoint_freq`: the number of epochs after which to save a model checkpoint\n",
    " - `weighted`: if true, then weighted loss is used\n",
    " - `resume_checkpoint`: if true, then training resumes from the checkpoint given by `eval_checkpoint`\n",
    " - `loss_function`: the loss function to be used\n",
    " - `evaluate_water`: if true, then an additional metric is computed for water and no water classes\n",
    " - `eval_checkpoint`: if given a path, then training resumes from the checkpoint in that path\n",
    " \n",
    " The `configs` dictionary must include all this information. We provide a function that reads all the required config files and updates the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b796a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train activations  27\n",
      "val activations  7\n",
      "test activations  10\n",
      "Configs updated\n"
     ]
    }
   ],
   "source": [
    "configs = update_config(configs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bdf6e-7bad-4fbb-aa12-66d33dacc466",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"train_pickle\"] = PROJ_ROOT / \"pickle/KuroV2_grid_dict.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ada30",
   "metadata": {},
   "source": [
    "### GRD Data loaders\n",
    "\n",
    "#### With DEM\n",
    "\n",
    "Now let's initialise the data loaders. For our first example we will use GRD with DEM data, so we will update the `configs` dictionary accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Initializing  RandomEvents\n",
      "====================\n",
      "(train) Calculating stats for dataset...\n",
      "Pickle file not found!  /u/mp005/challenge/KuroSiwo/pickle/KuroV2_grid_dict.pkl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslope\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/ellis-kuro-siwo-blue/KuroSiwo/utilities/utilities.py:92\u001b[0m, in \u001b[0;36mprepare_loaders\u001b[0;34m(configs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mSLCDataset(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, configs\u001b[38;5;241m=\u001b[39mconfigs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mDataset(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, configs\u001b[38;5;241m=\u001b[39mconfigs)\n\u001b[1;32m     94\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mDataset(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, configs\u001b[38;5;241m=\u001b[39mconfigs)\n",
      "File \u001b[0;32m~/Repositories/ellis-kuro-siwo-blue/KuroSiwo/dataset/Dataset.py:55\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, mode, configs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_valids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load precomputed min-max stats for each SAR image or calculate them anew\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_max_random_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_min_max_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclz_stats \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Repositories/ellis-kuro-siwo-blue/KuroSiwo/dataset/Dataset.py:502\u001b[0m, in \u001b[0;36mDataset.update_min_max_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     pickle_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 502\u001b[0m grids \u001b[38;5;241m=\u001b[39m \u001b[43mget_grids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickle_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m grids\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    505\u001b[0m     record \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Repositories/ellis-kuro-siwo-blue/KuroSiwo/dataset/Dataset.py:29\u001b[0m, in \u001b[0;36mget_grids\u001b[0;34m(pickle_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(pickle_path):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickle file not found! \u001b[39m\u001b[38;5;124m\"\u001b[39m, pickle_path)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pickle_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     31\u001b[0m     grid_dict \u001b[38;5;241m=\u001b[39m load(file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "configs[\"dem\"] = True\n",
    "configs[\"slope\"] = False\n",
    "configs[\"slc\"] = False\n",
    "train_loader, val_loader, test_loader = prepare_loaders(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa9065",
   "metadata": {},
   "source": [
    "When initialising the dataloaders for the first time, a `stats.pkl` file is produced which contains the minimum and maximum values of the dataset, useful for scaling data later on.\n",
    "\n",
    "In the screen prints we can also see the number of samples per Climatic Zone and per AOI for each set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d5f86",
   "metadata": {},
   "source": [
    "#### Sample visualization\n",
    "\n",
    "Let's visualize a sample from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_loader.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    post_scale_var_1,\n",
    "    post_scale_var_2,\n",
    "    post_event,\n",
    "    mask,\n",
    "    pre1_scale_var_1,\n",
    "    pre1_scale_var_2,\n",
    "    pre_event_1,\n",
    "    pre2_scale_var_1,\n",
    "    pre2_scale_var_2,\n",
    "    pre_event_2,\n",
    "    dem,\n",
    "    clz,\n",
    "    activ,\n",
    ") = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d97f62",
   "metadata": {},
   "source": [
    "Each batch contains the following data:\n",
    " - `post_scale_var_1`, `post_scale_var_2`: the variables used for scaling the post-flood image (useful for reverse scaling afterwards, if needed)\n",
    " - `post_event`: the post-flood image\n",
    " - `mask`: the ground truth mask\n",
    " - `pre1_scale_var_1`, `pre1_scale_var_2`:  the variables used for scaling the first pre-flood image (useful for reverse scaling afterwards, if needed)\n",
    " - `pre_event_1`: the first pre-flood image\n",
    " - `pre2_scale_var_1`, `pre2_scale_var_2`:  the variables used for scaling the second pre-flood image (useful for reverse scaling afterwards, if needed)\n",
    " - `pre_event_2`: the second pre-flood image\n",
    " - `dem`: the DEM\n",
    " - `clz`: an integer depicting the climate zone of the sample\n",
    " - `activ`: an integer depicting the activation ID of the sample\n",
    " \n",
    "Note that we had defined `data_configs['inputs'] = ['pre_event_1', 'pre_event_2', 'post_event']`, therefore the dataloader returns all three images of the sample.\n",
    "\n",
    "We have also defined `data_configs['channels'] = ['vv', 'vh']`, therefore each image contains two channels: VV and VH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6569d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_event.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46579b8d",
   "metadata": {},
   "source": [
    "Now let's visualise the images in this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8aa2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(20, 8))\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    \"Custom cmap\",\n",
    "    [\n",
    "        (0, 0, 0, 10),\n",
    "        (0.09019607843137255, 0.7450980392156863, 0.8117647058823529, 1.0),\n",
    "        (0.8647058823529412, 0.30980392156862746, 0.45882352941176474, 1.0),\n",
    "        (0.918, 0.929, 0.361, 1.0),\n",
    "    ],\n",
    "    4,\n",
    ")\n",
    "\n",
    "# First pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_1, torch.tensor(pre1_scale_var_1), torch.tensor(pre1_scale_var_2), configs\n",
    ")\n",
    "ax[0].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0].set_title(\"Pre 1 (VV)\")\n",
    "ax[1].set_title(\"Pre 1 (VH)\")\n",
    "\n",
    "# Second pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_2, torch.tensor(pre2_scale_var_1), torch.tensor(pre2_scale_var_2), configs\n",
    ")\n",
    "ax[2].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[3].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[2].set_title(\"Pre 2 (VV)\")\n",
    "ax[3].set_title(\"Pre 2 (VH)\")\n",
    "\n",
    "# Post-image\n",
    "img = reverse_scale_img(\n",
    "    post_event, torch.tensor(post_scale_var_1), torch.tensor(post_scale_var_2), configs\n",
    ")\n",
    "ax[4].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[5].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[4].set_title(\"Post (VV)\")\n",
    "ax[5].set_title(\"Post (VH)\")\n",
    "\n",
    "# DEM\n",
    "img = dem\n",
    "ax[6].imshow(img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "ax[6].set_title(\"DEM\")\n",
    "\n",
    "# Mask\n",
    "img = mask\n",
    "ax[7].imshow(img.squeeze().cpu().numpy(), cmap=cmap, vmin=0, vmax=3)\n",
    "ax[7].set_title(\"Mask\")\n",
    "\n",
    "for i in range(8):\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    ax[i].spines[[\"top\", \"bottom\", \"left\", \"right\"]].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac529da",
   "metadata": {},
   "source": [
    "The class labels in the mask follow this scheme:\n",
    " - `0`: 'No water'  (black)\n",
    " - `1`: 'Permanent Waters'  (blue)\n",
    " - `2`: 'Floods'  (red)\n",
    " - `3`: 'Invalid pixels'  (yellow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41bcd5",
   "metadata": {},
   "source": [
    "Now let's see how the slope information looks like. For this, we have to set the parameter `data_configs['slope'] = true` and initialise different dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"dem\"] = True\n",
    "configs[\"slope\"] = True\n",
    "configs[\"slope_path\"] = None\n",
    "configs[\"slc\"] = False\n",
    "train_loader, val_loader, test_loader = prepare_loaders(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_loader.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28909d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    post_scale_var_1,\n",
    "    post_scale_var_2,\n",
    "    post_event,\n",
    "    mask,\n",
    "    pre1_scale_var_1,\n",
    "    pre1_scale_var_2,\n",
    "    pre_event_1,\n",
    "    pre2_scale_var_1,\n",
    "    pre2_scale_var_2,\n",
    "    pre_event_2,\n",
    "    slope,\n",
    "    clz,\n",
    "    activ,\n",
    ") = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20430fb0-867c-4c06-ac81-4f4b1bed9630",
   "metadata": {},
   "source": [
    "Notice how the batch contents are the same. The only thing that changes is that instead of the DEM we now get the calculated slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f79e60-1047-4ccd-bacf-4b38e83c1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(20, 8))\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    \"Custom cmap\",\n",
    "    [\n",
    "        (0, 0, 0, 10),\n",
    "        (0.09019607843137255, 0.7450980392156863, 0.8117647058823529, 1.0),\n",
    "        (0.8647058823529412, 0.30980392156862746, 0.45882352941176474, 1.0),\n",
    "        (0.918, 0.929, 0.361, 1.0),\n",
    "    ],\n",
    "    4,\n",
    ")\n",
    "\n",
    "# First pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_1, torch.tensor(pre1_scale_var_1), torch.tensor(pre1_scale_var_2), configs\n",
    ")\n",
    "ax[0].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0].set_title(\"Pre 1 (VV)\")\n",
    "ax[1].set_title(\"Pre 1 (VH)\")\n",
    "\n",
    "# Second pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_2, torch.tensor(pre2_scale_var_1), torch.tensor(pre2_scale_var_2), configs\n",
    ")\n",
    "ax[2].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[3].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[2].set_title(\"Pre 2 (VV)\")\n",
    "ax[3].set_title(\"Pre 2 (VH)\")\n",
    "\n",
    "# Post-image\n",
    "img = reverse_scale_img(\n",
    "    post_event, torch.tensor(post_scale_var_1), torch.tensor(post_scale_var_2), configs\n",
    ")\n",
    "ax[4].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[5].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[4].set_title(\"Post (VV)\")\n",
    "ax[5].set_title(\"Post (VH)\")\n",
    "\n",
    "# SLOPE\n",
    "img = slope\n",
    "ax[6].imshow(img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "ax[6].set_title(\"Slope\")\n",
    "\n",
    "# Mask\n",
    "img = mask\n",
    "ax[7].imshow(img.squeeze().cpu().numpy(), cmap=cmap, vmin=0, vmax=3)\n",
    "ax[7].set_title(\"Mask\")\n",
    "\n",
    "for i in range(8):\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    ax[i].spines[[\"top\", \"bottom\", \"left\", \"right\"]].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58cf29-9812-4f3d-881e-2332713bc89f",
   "metadata": {},
   "source": [
    "### SLC Data Loaders\n",
    "\n",
    "Now let's visualise the SLC data in Kuro Siwo. To get the SLC component instead of the GRD, we have to update the appropriate paths in the configuration files. Note that DEM data are not included in the SLC component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa026cb-b382-44a3-864f-9ff8df744e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"slc\"] = True\n",
    "configs[\"train_json\"] = \"/u/mp005/challenge/KuroSiwo/json/slc_grid_pwater_0.0001.json\"\n",
    "configs[\"test_json\"] = \"/u/mp005/challenge/KuroSiwo/json/slc_grid_pwater_0.json\"\n",
    "configs[\"slc_root_path\"] = \"/ptmp/mp005/KuroSiwoSLC/\"\n",
    "\n",
    "configs[\"dem\"] = False\n",
    "configs[\"slope\"] = False\n",
    "train_loader, val_loader, test_loader = prepare_loaders(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53722c-541e-4b0c-b343-8aa947a54bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_loader.dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c60393-295d-4c24-8f9d-6caf4175044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    post_scale_var_1,\n",
    "    post_scale_var_2,\n",
    "    post_event,\n",
    "    mask,\n",
    "    pre1_scale_var_1,\n",
    "    pre1_scale_var_2,\n",
    "    pre_event_1,\n",
    "    pre2_scale_var_1,\n",
    "    pre2_scale_var_2,\n",
    "    pre_event_2,\n",
    "    clz,\n",
    "    activ,\n",
    ") = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b5016-ce12-44d7-adce-75c738be3227",
   "metadata": {},
   "source": [
    "Each SLC patch contains the following channels:\n",
    " - `Phase VH`\n",
    " - `Phase VV`\n",
    " - `Amplitude VH`\n",
    " - `Amplitude VV` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ebce8-2a98-46e7-8862-e1a19b7fc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_event.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bebc7-a28f-46bc-b38c-431a52400477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    \"Custom cmap\",\n",
    "    [\n",
    "        (0, 0, 0, 10),\n",
    "        (0.09019607843137255, 0.7450980392156863, 0.8117647058823529, 1.0),\n",
    "        (0.8647058823529412, 0.30980392156862746, 0.45882352941176474, 1.0),\n",
    "        (0.918, 0.929, 0.361, 1.0),\n",
    "    ],\n",
    "    4,\n",
    ")\n",
    "\n",
    "# First pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_1, torch.tensor(pre1_scale_var_1), torch.tensor(pre1_scale_var_2), configs\n",
    ")\n",
    "ax[0, 0].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 1].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 2].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 3].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 0].set_title(\"Pre 1 (Phase VV)\")\n",
    "ax[0, 1].set_title(\"Pre 1 (Phase VH)\")\n",
    "ax[0, 2].set_title(\"Pre 1 (Amplitude VV)\")\n",
    "ax[0, 3].set_title(\"Pre 1 (Amplitude VH)\")\n",
    "\n",
    "# Second pre-image\n",
    "img = reverse_scale_img(\n",
    "    pre_event_2, torch.tensor(pre2_scale_var_1), torch.tensor(pre2_scale_var_2), configs\n",
    ")\n",
    "ax[0, 4].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 5].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 6].imshow(img[2].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 7].imshow(img[3].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[0, 4].set_title(\"Pre 2 (Phase VV)\")\n",
    "ax[0, 5].set_title(\"Pre 2 (Phase VH)\")\n",
    "ax[0, 6].set_title(\"Pre 2 (Amplitude VV)\")\n",
    "ax[0, 7].set_title(\"Pre 2 (Amplitude VH)\")\n",
    "\n",
    "# Post-image\n",
    "img = reverse_scale_img(\n",
    "    post_event, torch.tensor(post_scale_var_1), torch.tensor(post_scale_var_2), configs\n",
    ")\n",
    "ax[1, 0].imshow(img[0].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1, 1].imshow(img[1].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1, 2].imshow(img[2].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1, 3].imshow(img[3].cpu().numpy() * 4, cmap=\"gray\")\n",
    "ax[1, 0].set_title(\"Post (Phase VV)\")\n",
    "ax[1, 1].set_title(\"Post (Phase VH)\")\n",
    "ax[1, 2].set_title(\"Post (Amplitude VV)\")\n",
    "ax[1, 3].set_title(\"Post (Amplitude VH)\")\n",
    "\n",
    "# Mask\n",
    "img = mask\n",
    "ax[1, 4].imshow(img.squeeze().cpu().numpy(), cmap=cmap, vmin=0, vmax=3)\n",
    "ax[1, 4].set_title(\"Mask\")\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(8):\n",
    "        ax[i, j].set_xticks([])\n",
    "        ax[i, j].set_yticks([])\n",
    "        ax[i, j].spines[[\"top\", \"bottom\", \"left\", \"right\"]].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98804510-5a57-4306-a04c-6c922e8fb13b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
